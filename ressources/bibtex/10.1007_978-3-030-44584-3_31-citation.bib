@InProceedings{10.1007/978-3-030-44584-3_31,
author="Overton, Toyah
and Tucker, Allan",
editor="Berthold, Michael R.
and Feelders, Ad
and Krempl, Georg",
title="DO-U-Net for Segmentation and Counting",
booktitle="Advances in Intelligent Data Analysis XVIII",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="391--403",
abstract="Many image analysis tasks involve the automatic segmentation and counting of objects with specific characteristics. However, we find that current approaches look to either segment objects or count them through bounding boxes, and those methodologies that both segment and count struggle with co-located and overlapping objects. This restricts our capabilities when, for example, we require the area covered by particular objects as well as the number of those objects present, especially when we have a large amount of images to obtain this information for. In this paper, we address this by proposing a Dual-Output U-Net. DO-U-Net is an Encoder-Decoder style, Fully Convolutional Network (FCN) for object segmentation and counting in image processing. Our proposed architecture achieves precision and sensitivity superior to other, similar models by producing two target outputs: a segmentation mask and an edge mask. Two case studies are used to demonstrate the capabilities of DO-U-Net: locating and counting Internally Displaced People (IDP) tents in satellite imagery, and the segmentation and counting of erythrocytes in blood smears. The model was demonstrated to work with a relatively small training dataset, achieving a sensitivity of 98.69{\%} for IDP camps of the fixed resolution, and 94.66{\%} for a scale-invariant IDP model. DO-U-Net achieved a sensitivity of 99.07{\%} on the erythrocytes dataset. DO-U-Net has a reduced memory footprint, allowing for training and deployment on a machine with a lower to mid-range GPU, making it accessible to a wider audience, including non-governmental organisations (NGOs) providing humanitarian aid, as well as health care organisations.",
isbn="978-3-030-44584-3"
}
