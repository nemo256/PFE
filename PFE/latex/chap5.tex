\section{Introduction}
\vspace{0.2in}
\hspace*{0.16in}
As part of our research, we have treated the case of segmenting and counting Red, White blood cells and platelets which also known as CBC (Complete Blood Count), we are using the ALL-IDB\textsuperscript{\cite{pm77-2n23-20}} Dataset to train and evaluate our models.
In our case study, and from multiple articles, we can see that U-Net and Segnet models are dominating the field of cell segmentation and Medical Computer vision in general.
In this chapter, we test out both U-Net and Segnet models, and analyse the results by comparing results of the two architectures.
We will also explore different machine learning algorithms for both preprocessing and postprocessing.

\section{Our approach}
\vspace{0.2in}
\hspace*{0.16in}
From all of the intel we have gathered, and previously read articles, all of cell segmentation (blood cell segmentation in particular) are mostly using U-Net and Segnet archtectures for segmenting blood cell images.
We have implemented both the U-Net and Segnet models.
In the following sections, we will briefly analyze and compare both convolutional neural network (CNN) models with their perspective results.
And explain all the postprocessing methods we used for the counting of blood cells (red, white and platelets)

\section{U-Net}
\subsection{Definition}
The U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. In our case we are using DO-UNet from \textsuperscript{\cite{10.1007/978-3-030-44584-3_31}} which is a modified U-Net to produce dual outputs, which also known as contour aware network was first demonstrated by the DCAN architecture \textsuperscript{\cite{chen2016dcan}}.  Based on a simple FCN, DCAN was trained to use the outer
contours of the areas of interest to guide the training of the segmentation masks. This led to improved semantic and instance segmentation of the model, which in their case, looked at non-overlapping features in biomedical imaging.
With the aim of counting closely co-located and overlapping cells, we are predominantly interested in the correct detection of individual objects as
opposed to the exact precision of the segmentation mask itself. An examination
of the hidden convolutional layers of the classical U-Net showed that the penultimate layer of the network extracts information about the edges of the cells, so the idea is to output the cell mask + edge mask then do a substraction to break the overlapping cells.

\subsection{Architecture}
They Started with the classical U-Net then reduced the number of
convolutional layers and skip connections in the model. Simultaneously, they minimised the complexity of the model by looking at smaller input regions of the images, thus minimising the memory footprint of the model. They follow the approach of Ronneberger et. al. \textsuperscript{\cite{10.1007/978-3-030-44584-3_31}} by using unpadded convolutions throughout the network, resulting in a model with smaller output edge and mask (100 × 100 px) corresponding to a central region of a larger (188 × 188 px) input image region. DO-U-Net uses two, independently trained, output layers of identical size. Figure \ref{fig:DO-UNET} shows the DO-U-Net architecture.

\begin{figure}[H]
\centering
  \vspace{-0.1in}
    \centerline{\includegraphics[width = \linewidth]{../images/DO-UNET.png}}
    \caption{DO-UNet architecture}
    \label{fig:DO-UNET}
\end{figure}

\section{Segnet}
\subsection{Definition}
The SegNet neural network, developed by Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla, all from the University of Cambridge, is a convolutional neural network used for semantic pixel wise labeling. This problem is more commonly called semantic segmentation. \textsuperscript{\cite{badrinarayanan2017segnet}}

\subsection{Architecture}
SegNet has an encoder network and a corresponding decoder network, followed by a final pixelwise classification layer. This architecture is illustrated in the below figure.
With 13 encoder layers obtained from the VGG16 network, and 13 decoder layers to match the same number of encoder layers. The final decoder output is fed to a multi-class soft-max classifier to produce class probabilities for each pixel independently (pixelwise).

Each encoder in the encoder network performs convolution with a filter bank to produce a set of feature maps. These are then batch normalized. Then an element-wise rectified- linear non-linearity (ReLU) max(0, x) is applied. Following that, max-pooling with a 2x2 window and stride 2 (non-overlapping window) is performed and the resulting output is sub-sampled by a factor of 2. Max-pooling is used to achieve translation invariance over small spatial shifts in the input image.

The appropriate decoder in the decoder network upsamples its input feature map(s) using the memorized max-pooling indices from the corresponding encoder feature map(s). This step pro- duces sparse feature map(s). This SegNet decoding technique is illustrated in the below figure.
These feature maps are then convolved with a trainable decoder filter bank to produce dense feature maps. A batch normalization step is then applied to each of these maps. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map, although its encoder input has 3 channels (RGB).
This is unlike the other decoders in the network which produces feature maps with the same number of size and channels as their encoder inputs. The high dimensional feature representation at the output of the final decoder is fed to a trainable soft-max classifier.
This soft-max classifies each pixel independently. The output of the soft-max classifier is a K channel image of probabilities where K is the number of classes. The predicted segmentation corresponds to the class with maximum probability at each pixel. \textsuperscript{\cite{badrinarayanan2017segnet}}

\vspace{0.2in}

\begin{figure}[H]
\centering
  \vspace{-0.1in}
    \centerline{\includegraphics[width = 4in, height = 2.2in]{../images/segnet.png}}
    \caption{SegNet architecture}
\end{figure}

\subsection{Dataset}
For the segnet model, we have decided to test th ALL-IDB1 dataset, which contains 108 blood cell images, of which 10 images with their perspective masks and edge masks were chosen for red blood cell training and 3 as a test dataset, for white blood cells 73 images with their masks, and 33 as a test dataset.
For platelets, we used 71 for training and 31 as a test dataset.
Only red blood cells have edge masks, because we need to get rid of overlapped cells, white blood cells and platelets dont need the edge masks, using masks only can retrieve all the necessary features, because both white blood cells and platelets rarely overlap.
The images will be sliced and rescaled to 128x128 to match the input shape of the Segnet model.
The resulting train dataset will be 3916 image, mask, and edge tiles (a total of 11748 tiles).
For the test dataset 1072 image, mask, and edge tiles (a total of 3216 tiles) for red blood cells.
As for white blood cells, 28126 image and mask tiles were used for training (a total of 56252 tiles), and 15892 image and mask tiles were used as a test dataset for white blood cells (a total of 31784 tiles).
Finally, for platelets we used 27650 image and mask tiles were used for training (a total of 55300), and 14410 image and mask tiles as a test dataset for platelets (a total of 28820).

\vspace{0.1in}

\input{tables/table5.tex}

\subsection{Dataset augmentation}
We used the same dataset augmention on all cells (red, white blood cells, and platelets).
The augmentation we used was custom which involves the following steps:
\begin{enumerate}
    \item Pick a random image from the train dataset.
    \item Get the x and y coordinates randomly from the chosen image.
    \item Rescale the image randomly.
    \item Find the edges of a box around the image chip and the mask chip.
    \item Take a slice of the image and mask accordingly.
    \item Skip empty image chips (masks, and edge chips for red blood cells).
    \item Resize the image and mask chip to 128x128.
    \item Randomly rotate and flip the image chip.
    \item Randomly augment the colors (luminosity and saturation).
    \item Rescale the chip back to normal.
\end{enumerate}

\section{Counting}
\vspace{0.2in}
\hspace*{0.16in}
After having segmented blood cell images (red, white blood cells and platelets), we use many postprocessing methods (machine learning algorithms) to get the coordinates of circles and count them.
Here are all the algorithms we used to get an relatively accurate blood cell count:

\subsection{Circle Hough Transform}
Circle Hough Transform (CHT) is machine learning algorithm used to extract features (circles) from imperfect images.
We modified its parameters (Minimum distribution, Minimum and Maximum radius...) for each type of blood cells (red, white and platelets).\\
Note that we do not rely on this approach to count white blood cells, because most white blood cells have different shapes.
Therefore, this method is useless when it comes to white blood cells counting.

\subsection{Connected Component Labeling}
Connected Component Labeling (CCL) is machine learning algorithm used to detect connected regions in a binary image.
Before applying the connected component labeling, we convert the images to grayscale. Then, a binary threshold is applied to the images to get binary values.
Finally, we apply the connected component labeling to get the labels and map component labels to the resulting image.

\subsection{Watershed}
Watershed algorithms (also called drainage divide) are used in image processing primarily for object segmentation purposes, that is, for separating different objects in an image. the main purpose of using watershed in this phase is to segment the touching and overlapping cells, the watershed takes two inputs, first i takes an image with different intensity levels in our-case the distance transform of our mask where the intensity levels represents reliefs. the second input is the water sources in our-case we extracted local maxima from the distance transform image. we can see below the steps we used to count the cells.

\begin{enumerate}
    \item \textbf{Compute the Euclidean distance}: we compute euclidean distance from every binary pixel to the nearest zero pixel, this map will be used as our relief map in the watershed algorithm.
    \item \textbf{We find peaks in the distance map}: we search for peaks in our euclidean distance map which is the local maxima in each region, which are the highest points in the map (higher intensity levels), which we will use as water sources in the watershed algorithm.
    \item \textbf{apply connected component labeling on the peak map}: we apply CCL algorithm which is also called 8-connectivity algorithm to label the peaks (label each water source).
    \item \textbf{apply the Watershed algorithm on the reversed distance map using the labeled peaks}: at the end we feed the reversed distance map and the water sources map (local maxima) to the watershed algorithm to get the segmented image. 
\end{enumerate}

\section{Metrics And Loss Functions}
Loss functions are one of the important ingredients in deep learning-based medical image segmentation methods. In the past four years, more than 20 loss functions have been proposed for various segmentation tasks. Most of them can be used in any segmentation tasks in a plug-and-play way, we can see in fig \ref{fig:LossFunctions} the relations between the most used  Loss Functions, we will present below the used Loss Functions in our paper.

\begin{figure}[H]
\centering
  \vspace{-0.1in}
    \centerline{\includegraphics[width = \linewidth]{../images/LossFunctions.png}}
    \caption{Loss Functions}
\end{figure}
In this section, we will discuss the metrics that we used to evaluate our models.

When performing classification predictions (pixel-wise classification in our case) there's four types of outcomes that could occur.

\begin{enumerate}
    \item \textbf{True positives} are when you predict an observation belongs to a class and it actually does belong to that class.
    \item \textbf{True negatives} are when you predict an observation does not belong to a class and it actually does not belong to that class.
    \item \textbf{False positives} occur when you predict an observation belongs to a class when in reality it does not.
    \item \textbf{False negatives} occur when you predict an observation does not belong to a class when in fact it does.
\end{enumerate}

These four outcomes are often plotted on a confusion matrix. The following confusion matrix is an example for the case of binary classification. This matrix should be generated making predictions on the test data and then identifying each prediction as one of the four possible outcomes described above.

\vspace{0.1in}

\input{tables/table6.tex}

The three main metrics used to evaluate a classification model are accuracy, precision, and recall.

Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.

accuracy=correctpredictionsallpredictions

Precision is defined as the fraction of relevant examples (true positives) among all of the examples which were predicted to belong in a certain class.

precision=truepositivestruepositives+falsepositives

Recall is defined as the fraction of examples which were predicted to belong to a class with respect to all of the examples that truly belong in the class.

recall=truepositivestruepositives+falsenegatives

We have a semantic segmentation problem. Therefore, we use the following metrics:

\subsection{Pixel Accuracy}
Pixel accuracy is perhaps the easiest to understand conceptually. It is the percent of pixels in the input image that are classified correctly.


We don't rely on this metric because it is susceptible to class-imbalance, which is when the classes are extremely imbalanced, it means that a class or some classes dominate the image, while some other classes make up only a small portion of the image. Unfortunately, class imbalance is prevalent in many real world data sets, so it can’t be ignored. To further illustrate this, if an input image was 100\% black, the output prediction would be above 90\% accurate, which is a totally false prediction.


\subsection{IOU}
The Jaccard Index or Intersection Over Union, also known as the Jaccard similarity coefficient, is a statistic (metric) used for gauging the similarity and diversity of sample sets. It was developed by Grove Karl Gilbert in 1884 as his ratio of verification (v),\textsuperscript{\cite{murphy1996finley}} and now is frequently referred to as the Critical Success Index in meteorology. It was later developed independently by Paul Jaccard, originally giving the French name coefficient de communauté.\textsuperscript{\cite{jaccard1912distribution}} The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets, here is the formula:

\begin{equation}
    J(A,B) = \frac{Area\; of\; Overlap}{Area\; of\; Union} = \frac{|A \cap B|}{|A \cup B|}
\end{equation}

Here is an example of using IOU on a stop sign, where the green bounding box is the ground truth (the right prediction) and the red bounding box is what the model predicted.

\vspace{0.1in}

\begin{figure}[H]
\centering
  \vspace{-0.1in}
    \centerline{\includegraphics[width = 4in, height = 2.5in]{../images/exampleIOU.jpg}}
    \caption{Example of IOU applied on a stop sign image}
\end{figure}


\subsection{Dice}
The Sørensen–Dice coefficient is a statistic used to measure the similarity of two samples, It was developed by the botanists (scientific study of plants) Thorvald Sørensen and Lee Raymond Dice, who published in 1948 and 1945 respectively.\\
Sørensen's original formula was intended to be applied to discrete data. Given two sets, X and Y, it is defined as :
\begin{equation}
    DSC(X, Y) = \frac{2 | X \cap Y |}{| X |  +  | Y |}
\end{equation}
where |X| and |Y| are the cardinalities of the two sets . The Sørensen index equals twice the number of elements common to both sets divided by the sum of the number of elements in each set as we can see in fig \ref{fig:DSC_EX}. 
When applied to Boolean data, using the definition of true positive (TP), false positive (FP), and false negative (FN), it can be written as :
\begin{equation}
    DSC = \frac{2 TP}{2 TP + FN + FP}
\end{equation}

\begin{figure}[H]
\centering
  \vspace{-0.1in}
    \centerline{\includegraphics[width = 4in, height = 2.5in]{../images/DSC.png}}
    \caption{Example explaining Dice coefficient}
    \label{fig:DSC_EX}
\end{figure}


This coefficient is not very different in form from the Jaccard index (IOU). In fact, both are equivalent in the sense that given a value for the Sørensen–Dice coefficient $S$ , one can calculate the respective Jaccard index value $J$  and vice versa, using the equations : 
\begin{equation}
    J = \frac{DSC}{( 2 − DSC )}
\end{equation}

and 

\begin{equation}
    DSC = \frac{2 J  }{( 1 + J )}
\end{equation}

The function ranges between zero and one, like Jaccard. the corresponding loss function:
\begin{equation}
    DSC = 1 - \frac{2 TP}{2 TP + FN + FP}
\end{equation}

\subsection{Tversky}






\section{Conclusion}
\vspace{0.2in}
\hspace*{0.16in}

\newpage
