\vspace*{\fill}
\begin{center}
    {\color{Black} \rule{\linewidth}{1.2mm} }\\
\vspace{0.25in}
{\centering\fontsize{30}{40}{\bfseries{\color{Black}{\scshape{Chapter VI : Experiments And Results}}}}}
\vspace{0.35in}\\
    {\color{Black} \rule{\linewidth}{1.2mm} }
\end{center}
\vspace*{\fill}
\addcontentsline{toc}{chapter}{\color{Black}{Chapter VI : Experiments And Results}}
\setcounter{section}{0}

\newpage

\section{Introduction}
\vspace{0.2in}
\hspace{\parindent}
In this Section we will detail the different experiments and we will discuss the evaluation results obtained from the two segmentation models DO-UNet and DO-SegNet both with the BinaryCrossEnropy Loss Function. And we also discuss the evaluation results of the three counting algorithms Watershed, Circle Hough Transform and Connected Component Labeling for the three blood elements RBCs and WBCs and Platelets.

\section{Used Tools}
\subsection{TensorFlow}
\hspace{\parindent}
In our work we used TensorFlow which is an open source library developed by Google Brain Team for Artificial Intelligence, it contains multiple pre-defined models and algorithms for Deep Learning and Machine Learning, tensorFlow can be used in multiple languages as python, C++, Java and JavaScript.\\
Tensorflow can be used to Create, Train, Deploy Models. And when we talk about complicated models we can use Keras.
Keras is a high level API for Neural Networks which helps with experiments on the models and extensibility.

\begin{figure}[H]
    \centering
      \vspace{-0.1in}
        \centerline{\includegraphics[width = 2.5in]{../images/tensorflow.png}}
        \caption{Tensorflow}
        \label{Tensorflow}
    \end{figure}

\subsection{Colab}
\hspace{\parindent}
Google Colab or Colaboratory is a free Jupyter notebook environment running on Google's cloud servers for machine learning training and research. This platform allows the user to leverage backend hardware such as GPUs and TPUs and train Machine Learning and Deep Learning models directly in the cloud. Without the need to install anything on our computer at the anything on our computer except a browser.
but it has some disadvantages where we have a limit on the GPU usage
and non presistant storage.

\begin{figure}[H]
\centering
  \vspace{-0.1in}
    \centerline{\includegraphics[width = 2in]{../images/colab.png}}
    \caption{Google Colab}
    \label{Google Colab}
\end{figure}

\subsection{PaperSpace}
\hspace{\parindent}
Paperspace is a high-performance cloud computing and ML development platform for building, training and deploying machine learning models. It has a complete jupyter notebook environement which has a persistent storage and 6 hours limit for each execution.

\begin{figure}[H]
    \centering
      \vspace{-0.1in}
        \centerline{\includegraphics[width = 3in]{../images/paperspace.png}}
        \caption{Paperspace}
        \label{Paperspace}
    \end{figure}

\section{do-U-Net Results}
\subsection{Red Blood Cells}
\hspace{\parindent}
The Red Blood Cells are the most difficult to detect because of the overlapping, where in some samples we can't notice the overlapping by eyes, in this experiment we are testing do-U-Net from \textsuperscript{\cite{10.1007/978-3-030-44584-3_31}}.
In the do-U-Net we updated the data augmentation phase. and applied Transfer Learning to get better edge mask. we can see in the dataset that we have only 13 images out of 108 from ALL-IDB1 that contains edge-masks and 108 masks, so the problem here is the lack of the edge label. we ended up with the method below as the best fit to our problem:

\begin{enumerate}
    \item Train the DO-Unet outputs on the big dataset (108 masks) which will output two identical masks.
    \item Continue Training the DO-Unet with the small dataset (13 masks, 13 edges), and Freeze the Mask Output.
\end{enumerate}

\input{tables/DO-UNet-Training.tex}

The table \ref{table:do-unet-transfer} compares between the normal trained model A on the small dataset which contains 13 masks and edges and the model be which is trained on two phases, first with the big dataset (108 mask without edge) for 60 epochs, in the second phase we continued training with the small dataset (13 masks + edges) for 400 epochs.
We can see that this method pushed the edge accuracy which is really important to get rid of the overlapping.

After training the dual-output-U-Net model we did a benchmark on the 13 images to calculate the counting accuracy of the three methods, we ended up with the table \ref{table:DOUNET-RBC} below where we can see that the Circle hough transform method is the best for RBC counting with 95.36 accuracy, because all the RBCs have a similar shape and size.

\input{tables/RBC_DOUNET.tex}

\subsection{White Blood Cells}
\hspace{\parindent}
The White Blood Cells are also difficult to detect because of the non stable shape and in some cases they overlap, in this experiment we are comparing single output U-Net from \textsuperscript{\cite{10.1007/978-3-030-44584-3_31}} and the single output SegNet model.
In the UNet we removed the edge output because we don't have the edge annotation. then trained the model for 15 epochs with the binaryCrossEntropy Loss function on (74 + 34) images. 
We ended up with a very high accuracy and IOU score as we can see in table \ref{table:unet-wbc-training}.

\input{tables/UNet-WBC-Training.tex}

After training the model we did a benchmark on the 13 images then on the complete database (108 images) to calculate the counting accuracy of the three methods.\\ 
The real count of the 108 images is calculated manually because the original database dosn't have the count information. We ended up with the table \ref{table:UNet-WBC} below where we can see that the watershed method is the best for WBC counting with 97.94 accuracy on the 13 images and 95.64 on the complete database, because the white blood cells always slightly overlap each other where it's easy to the watershed to segment them.

\input{tables/WBC_DOUNET.tex}

\subsection{Platlets}
\hspace{\parindent}
The Platlets are easy to count because of the rare overlapping but they are a bit difficult to segment because of their small size.\\
In this experiment we are testing single output U-Net from \textsuperscript{\cite{10.1007/978-3-030-44584-3_31}}. In the UNet we removed the edge output because we don’t have the edge annotation.
then trained the model for 50 epochs with the BinaryCrossEntropy Loss function on (74 + 34) images.
We ended up with a very high accuracy and IOU score as we can see in table \ref{table:PLT_DOUNET_TRAIN}.

\input{tables/PLT_UNET_Train.tex}

After training the model we did a benchmark on the 13 images to calculate the counting accuracy of the three methods.
We are comparing to the real count wich is calculated by feeding the ground truth platlets masks to  the CCL algorithm. because the original database dosn't have the count information.\\
We ended up with the table \ref{table:PLT_UNET_RESULT} below where we can see that the CCL method is the best for WBC counting with 98.58 accuracy, because of the rare overlapping on each other where it’s easy to the CCL to segment them.

\input{tables/PLT_DOUnet.tex}

\section{SegNet Results}
\hspace{\parindent}
SegNet segmentation results were pretty accurate for white blood cells and platelets, as for red blood cells, the segmentation was done using dual output (mask and edge-mask) to get rid of overlapped cells.\\
Here are the results of the Mean Squared Error (MSE) loss function on each type of cell:

\input{tables/segnet.tex}

\subsection{Red Blood Cells}
\hspace{\parindent}
For the dual-output SegNet model, the resulting segmented images were very good, sometimes better than the do-U-Net, though it is not as optimized when training and also predicting images, but it gets the job done with 95.86\% mask and 93.99\% edge accuracies. The segmented output images also had some noise which affected Connected Component Labeling (CCL) when counting.
As for Circle Hough Transform (CHT), the noise did not affect the result.
Red Blood Cells detection and counting is by far the hardest, because it is the only cell that overlaps and that makes it hard for counting.
The segmented output of do-SegNet is thresholded using a binary threshold, and then sent to 3 algorithms:

\begin{itemize}
  \item \textbf{Circle Hough Transform}: CHT was our best result for red blood cells counting, which achieved an accuracy of 94.03\% on the same dataset used for training the model (13 images with their respective masks and edge-masks).
  \item \textbf{Connected Component Labeling}: CCL was applied directly on the thresholded output edge, this method was far from accurate because the do-SegNet output had some noise (even when removing most of it), and also the overlapped nature of red blood cells which makes it very hard for this algorithm to count correctly. CCL achived an accuracy of 76.49\% counting red blood cells.
  \item \textbf{Euclidean Distance Transform}: EDT is used to get rid of the overlapped cells, also peak local max was applied on the EDT output for finding local maxima(s), the result of this approach is 84.64\% accuracy.
\end{itemize}

\input{tables/rbc_segnet.tex}

\subsection{White Blood Cells}
\hspace{\parindent}
The results of white blood cells segmentation and counting using the SegNet model was very accurate achieving 99.72\% when segmenting.
White blood cells are the easiest out of the three, and the most accurate results.
However, white blood cells are different from the other cells because they come in different shapes and sizes, which made it hard to adapt each counting algorithm to every cell.\
The same counting methods are applied CHT, CCL and EDT. And each method had some drawbacks.\\
Here are the results:

\begin{itemize}
  \item \textbf{Circle Hough Transform}: Due to the different shapes of white blood cells, CHT achieved the lowest result which is 79.9\% counting accuracy, because some of the cells don't even look like circles and also their differet size which made it harder to count.
  \item \textbf{Connected Component Labeling}: CCL is similiar to CHT when it comes to white blood cells. And, because of the noisy outputs of SegNet the binary threshold can only do so much (it thresholdes some of the noise generated when predicting).\\
    CCl achieved an average counting accuracy of 82.89\%.
  \item \textbf{Euclidean Distance Transform}: EDT is the contender of white blood cells counting, because the distance transform gets rid of the noise completely and peak local max was very helpfull in eliminating that noise and getting an accurate count.\\
    This method achived a counting accuracy of 96.43\%.
\end{itemize}

Here are the results of the 13 images:

\input{tables/wbc_segnet.tex}

\subsection{Platlets}
\hspace{\parindent}
The platelets segmentation result we achieved is not the best compared to the do-U-Net model. Segnet extracts the platelets but with some noise which made it very hard to count accurately.
It achieved a segmetation accuracy of xx.xx\% and the highest counting accuracy is 71.56\% which is not very good.\\
Here are all the counting accuracies for each approach:

\input{tables/plt_segnet.tex}

\section{Results}
\vspace{0.2in}
\hspace{\parindent}

\section{Conclusion}

\newpage
